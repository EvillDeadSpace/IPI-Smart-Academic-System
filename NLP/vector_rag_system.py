"""
IPI Akademija - RAG System sa Vector Embeddings
Implementacija kompletnog RAG (Retrieval-Augmented Generation) sistema

Plan implementacije:
1. Kreiranje vector embeddings za sve sekcije
2. ƒåuvanje u lokalnoj bazi ili Pinecone
3. Semantic search umesto keyword matching
4. Pobolj≈°an response generation
"""

import os
import json
import pickle
import numpy as np
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

class IPIVectorRAG:
    def __init__(self, use_pinecone=False):
        self.use_pinecone = use_pinecone
        self.knowledge_chunks = []
        self.embeddings = []
        self.metadata = []
        
        if use_pinecone:
            self.setup_pinecone()
        else:
            self.setup_local_storage()
    
    def setup_pinecone(self):
        """Setup Pinecone vector database"""
        try:
            import pinecone
            from pinecone import Pinecone, ServerlessSpec
            
            # Initialize Pinecone
            pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
            
            index_name = "ipi-akademija"
            
            # Create index if it doesn't exist
            if index_name not in pc.list_indexes().names():
                pc.create_index(
                    name=index_name,
                    dimension=1536,  # OpenAI text-embedding-3-small dimension
                    metric='cosine',
                    spec=ServerlessSpec(
                        cloud='aws',
                        region='us-east-1'
                    )
                )
            
            self.index = pc.Index(index_name)
            print("‚úÖ Pinecone setup uspe≈°an")
            
        except Exception as e:
            print(f"‚ùå Pinecone gre≈°ka: {e}")
            print("üîÑ Prebacujem na lokalno ƒçuvanje...")
            self.use_pinecone = False
            self.setup_local_storage()
    
    def setup_local_storage(self):
        """Setup lokalnog ƒçuvanja embeddings"""
        self.vector_db_path = "vector_database.pkl"
        self.metadata_path = "vector_metadata.json"
        print("‚úÖ Lokalno ƒçuvanje konfigurisano")
    
    def load_and_chunk_content(self):
        """Uƒçitava i deli sadr≈æaj na chunksove"""
        try:
            with open('fakultetski_sadr≈æaj.txt', 'r', encoding='utf-8') as f:
                content = f.read()
            
            chunks = []
            current_chunk = ""
            current_section = ""
            
            for line in content.split('\n'):
                line = line.strip()
                
                if line.startswith('##'):  # Sekcija
                    if current_chunk:
                        chunks.append({
                            'text': current_chunk.strip(),
                            'section': current_section,
                            'type': 'section'
                        })
                    current_section = line.replace('##', '').strip()
                    current_chunk = line + '\n'
                    
                elif line.startswith('-') and len(line) > 30:  # Bullet point
                    chunks.append({
                        'text': line.replace('-', '').strip(),
                        'section': current_section,
                        'type': 'bullet'
                    })
                    
                elif len(line) > 50:  # Dugaƒçka reƒçenica
                    chunks.append({
                        'text': line,
                        'section': current_section,
                        'type': 'paragraph'
                    })
            
            self.knowledge_chunks = chunks
            print(f"‚úÖ Kreiran {len(chunks)} chunksova")
            return chunks
            
        except Exception as e:
            print(f"‚ùå Gre≈°ka pri uƒçitavanju: {e}")
            return []
    
    def generate_embeddings(self):
        """Generi≈°e embeddings za sve chunksove"""
        if not self.knowledge_chunks:
            print("‚ùå Nema chunksova za embedding")
            return
        
        try:
            from openai import OpenAI
            
            client = OpenAI(
                base_url="https://models.inference.ai.azure.com",
                api_key=os.getenv("OPEN_API_KEY_OPENAI"),
            )
            
            print(f"üîÑ Generiram embeddings za {len(self.knowledge_chunks)} chunksova...")
            
            for i, chunk in enumerate(self.knowledge_chunks):
                print(f"Obraƒëujem chunk {i+1}/{len(self.knowledge_chunks)}")
                
                # Generi≈°i embedding
                response = client.embeddings.create(
                    input=[chunk['text']],
                    model="text-embedding-3-small",
                )
                
                embedding = response.data[0].embedding
                
                if self.use_pinecone:
                    # Store u Pinecone
                    self.index.upsert([{
                        'id': f"chunk_{i}",
                        'values': embedding,
                        'metadata': {
                            'text': chunk['text'],
                            'section': chunk['section'],
                            'type': chunk['type']
                        }
                    }])
                else:
                    # Store lokalno
                    self.embeddings.append(embedding)
                    self.metadata.append({
                        'id': f"chunk_{i}",
                        'text': chunk['text'],
                        'section': chunk['section'],
                        'type': chunk['type']
                    })
            
            if not self.use_pinecone:
                self.save_local_embeddings()
            
            print("‚úÖ Embeddings uspe≈°no generirani!")
            
        except Exception as e:
            print(f"‚ùå Gre≈°ka pri generisanju embeddings: {e}")
    
    def save_local_embeddings(self):
        """ƒåuva embeddings lokalno"""
        try:
            # ƒåuvaj embeddings
            with open(self.vector_db_path, 'wb') as f:
                pickle.dump(self.embeddings, f)
            
            # ƒåuvaj metadata
            with open(self.metadata_path, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            
            print("‚úÖ Embeddings saƒçuvani lokalno")
            
        except Exception as e:
            print(f"‚ùå Gre≈°ka pri ƒçuvanju: {e}")
    
    def load_local_embeddings(self):
        """Uƒçitava lokalne embeddings"""
        try:
            if os.path.exists(self.vector_db_path) and os.path.exists(self.metadata_path):
                with open(self.vector_db_path, 'rb') as f:
                    self.embeddings = pickle.load(f)
                
                with open(self.metadata_path, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                
                print(f"‚úÖ Uƒçitano {len(self.embeddings)} embeddings iz lokalne baze")
                return True
            return False
            
        except Exception as e:
            print(f"‚ùå Gre≈°ka pri uƒçitavanju: {e}")
            return False
    
    def semantic_search(self, query, top_k=3):
        """Semantic pretraga kroz embeddings"""
        try:
            from openai import OpenAI
            
            client = OpenAI(
                base_url="https://models.inference.ai.azure.com",
                api_key=os.getenv("OPEN_API_KEY_OPENAI"),
            )
            
            # Generi≈°i query embedding
            response = client.embeddings.create(
                input=[query],
                model="text-embedding-3-small",
            )
            query_embedding = response.data[0].embedding
            
            if self.use_pinecone:
                # Pretra≈æi Pinecone
                results = self.index.query(
                    vector=query_embedding,
                    top_k=top_k,
                    include_metadata=True
                )
                
                return [{
                    'text': match.metadata['text'],
                    'section': match.metadata['section'],
                    'score': match.score
                } for match in results.matches]
                
            else:
                # Pretra≈æi lokalno
                similarities = []
                for i, embedding in enumerate(self.embeddings):
                    similarity = np.dot(query_embedding, embedding) / (
                        np.linalg.norm(query_embedding) * np.linalg.norm(embedding)
                    )
                    similarities.append((similarity, i))
                
                # Sortiraj po sliƒçnosti
                similarities.sort(reverse=True)
                
                results = []
                for similarity, idx in similarities[:top_k]:
                    results.append({
                        'text': self.metadata[idx]['text'],
                        'section': self.metadata[idx]['section'],
                        'score': float(similarity)
                    })
                
                return results
                
        except Exception as e:
            print(f"‚ùå Gre≈°ka pri pretra≈æi: {e}")
            return []
    
    def generate_rag_response(self, query):
        """Generi≈°e RAG odgovor"""
        try:
            # Semantic search
            relevant_chunks = self.semantic_search(query, top_k=3)
            
            if not relevant_chunks:
                return "Izvinjavam se, ne mogu da pronaƒëem relevantne informacije."
            
            # Kreiraj kontekst
            context = "\n\n".join([chunk['text'] for chunk in relevant_chunks])
            
            # Generi≈°i odgovor
            from mistralai import Mistral, UserMessage, SystemMessage
            
            client = Mistral(
                api_key=os.getenv("OPEN_API_KEY_MISTRAL"), 
                server_url="https://models.inference.ai.azure.com"
            )
            
            system_prompt = f"""Ti si AI asistent za IPI Akademiju Tuzla.

RELEVANTNI KONTEKST:
{context}

INSTRUKCIJE:
- Odgovaraj na bosanskom jeziku
- Koristi samo informacije iz konteksta
- Budi precizan i informativan
- Ako nema≈° informaciju u kontekstu, reci da ne zna≈°
"""

            response = client.chat.complete(
                model="Mistral-small",
                messages=[
                    SystemMessage(content=system_prompt),
                    UserMessage(content=query),
                ],
                temperature=0.3,
                max_tokens=800,
            )
            
            return {
                'answer': response.choices[0].message.content,
                'sources': relevant_chunks,
                'context_used': context
            }
            
        except Exception as e:
            print(f"‚ùå Gre≈°ka pri generisanju odgovora: {e}")
            return "Izvinjavam se, do≈°lo je do gre≈°ke."

def create_rag_system(use_pinecone=False):
    """Kreira kompletan RAG sistem"""
    print("üöÄ Kreiram IPI Akademija RAG sistem...")
    
    rag = IPIVectorRAG(use_pinecone=use_pinecone)
    
    # Uƒçitaj postojeƒáe embeddings ili kreiraj nove
    if not use_pinecone and rag.load_local_embeddings():
        print("‚úÖ Koristim postojeƒáe embeddings")
    else:
        print("üîÑ Kreiram nove embeddings...")
        rag.load_and_chunk_content()
        rag.generate_embeddings()
    
    return rag

if __name__ == "__main__":
    # Test bez Pinecone (lokalno)
    print("üß™ TESTIRANJE LOKALNOG RAG SISTEMA")
    print("="*50)
    
    rag = create_rag_system(use_pinecone=False)
    
    # Test pitanja
    test_queries = [
        "Koje studijske programe ima IPI Akademija?",
        "Ko su profesori?",
        "Koliko ko≈°ta studij?",
        "Kako se upisati?"
    ]
    
    for query in test_queries:
        print(f"\n‚ùì PITANJE: {query}")
        print("-" * 30)
        
        result = rag.generate_rag_response(query)
        if isinstance(result, dict):
            print(f"ü§ñ ODGOVOR: {result['answer']}")
            print(f"üìä K√ÑLLOR: {len(result['sources'])} relevantnih chunksova")
        else:
            print(f"ü§ñ ODGOVOR: {result}")
